{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Lab5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nursimadonuk/CS-Capstone-Project/blob/main/Copy_of_Lab5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kqcX2Plvos-n"
      },
      "source": [
        "GloVe embeddings: https://nlp.stanford.edu/projects/glove/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jy6FTRg6nUek"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDDucswhnpgG"
      },
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFHMUXV_oD9O"
      },
      "source": [
        "model_glove_twitter = api.load(\"glove-twitter-25\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eew-48YwoG0P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "855f51e5-96c6-49e0-8c5d-f7ed983ff3ee"
      },
      "source": [
        "model_glove_twitter.wv.most_similar(\"good\",topn=10) #list of postitive: well, nice, better"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('too', 0.9648016095161438),\n",
              " ('day', 0.9533665180206299),\n",
              " ('well', 0.9503172039985657),\n",
              " ('nice', 0.9438973069190979),\n",
              " ('better', 0.9425961375236511),\n",
              " ('fun', 0.9418926239013672),\n",
              " ('much', 0.9413353204727173),\n",
              " ('this', 0.9387556314468384),\n",
              " ('hope', 0.9383507370948792),\n",
              " ('great', 0.9378515481948853)]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGd9Pg19-3_j",
        "outputId": "6ad5fb83-add4-4630-a153-95e7dac61182"
      },
      "source": [
        "model_glove_twitter.wv.most_similar(\"bad\",topn=10) #list of negative: damn, hell, crazy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('shit', 0.9544215202331543),\n",
              " ('crazy', 0.9532092809677124),\n",
              " ('but', 0.9522292017936707),\n",
              " ('hell', 0.9521805047988892),\n",
              " ('right', 0.9486410617828369),\n",
              " ('like', 0.9483203291893005),\n",
              " ('same', 0.9475184679031372),\n",
              " ('damn', 0.94697105884552),\n",
              " ('thing', 0.9445645213127136),\n",
              " ('way', 0.9423031210899353)]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvSn1VNwrG2Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702dcc6c-5ab2-4ffb-d053-a010e37da680"
      },
      "source": [
        "vec1 = model_glove_twitter[\"good\"]\n",
        "print(vec1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[-0.54403   0.60274  -0.14543  -0.023398 -0.13771   0.60137   2.192\n",
            "  0.20804  -0.51536  -0.23101  -0.80387   0.56901  -5.0234    0.26507\n",
            "  0.47891  -0.59854   0.56132  -1.0905   -0.52587   0.12506  -0.22624\n",
            "  0.24529  -0.45767   0.92619   0.022125]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpvGssfRIm-f"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375
        },
        "id": "RvQYJJ2II1m7",
        "outputId": "f026b15a-df87-4751-c31e-4d30353dafc3"
      },
      "source": [
        "#part 2 taking 5 positive tweets and making them a word vector\n",
        "#Zachary Motassim\n",
        "def SimpleTokenizer(tweet):\n",
        "  return tweet.split()\n",
        "'''\n",
        "Convert each tweet to a vector representation.\n",
        "- First get the embedding of each word in the tweet\n",
        "- Then average the word vectors together to create a single combined vector for the tweet\n",
        "- You should create 10 tweet vectors, one per tweet\n",
        "'''\n",
        "tweet_1=\"baby when i tell you God is real he is real and i’m so thankful for him\"\n",
        "tweet_2=\"The faster you remove people who just  take up space in your life the sooner you’ll find the ones who’d love to be there\"\n",
        "tweet_3=\"the feminine urge to tell every girl they’re pretty\"\n",
        "tweet_4=\"just plugged in my space heater I am unstoppable now\"\n",
        "tweet_5=\"There are 33 days until the Halo Infinite launch\"\n",
        "tokenized_tweet_1=SimpleTokenizer(tweet_1)\n",
        "tokenized_tweet_2=SimpleTokenizer(tweet_2)\n",
        "tokenized_tweet_3=SimpleTokenizer(tweet_3)\n",
        "tokenized_tweet_4=SimpleTokenizer(tweet_4)\n",
        "tokenized_tweet_5=SimpleTokenizer(tweet_5)\n",
        "\n",
        "for word in tokenized_tweet_1:\n",
        "  model_glove_twitter.wv.most_similar(word,topn=10)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-e566750a5e59>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtokenized_tweet_1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mmodel_glove_twitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[0;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m                 \u001b[0mmean\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    450\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"word 'God' not in vocabulary\""
          ]
        }
      ]
    }
  ]
}